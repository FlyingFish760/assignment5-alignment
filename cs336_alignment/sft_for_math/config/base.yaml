# Optimizer 
betas: [0.9, 0.99]
weight_decay: 1.0e-5
eps: 1.0e-8

# lr
max_lr: 1.0e-4
warmup_ratio: 0.1
cosine_ratio: 0.9

# Gradient
micro_batch_size: 4
grad_accumulation_steps: 8
grad_clip_norm: 1.0

# wandb setting
wandb_logging:
  use_wandb: True
  wandb_team: "cs336_assign5"
  wandb_project: "SFT_for_MATH"
  wandb_run: "sample_3496_lr_1e-4_bs_32"
  # config:
  #   max_learning_rate: 1.0e-4
  #   model_size: "test model"

vllm_seed: 42

n_samples: 3496